{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below and download the gutenburg package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size is 4323057 and test size is 9497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cntk\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "kjv = gutenberg.raw('bible-kjv.txt')\n",
    "chars = list(set(kjv))\n",
    "feature_size = len(chars)\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# split data into traning and testing\n",
    "test = kjv[3324518:3334015] # Malachi\n",
    "train = kjv[:3324518] + kjv[3334015:] # everything else\n",
    "print(\"training size is {} and test size is {}\".format(len(train),len(test)))\n",
    "\n",
    "def featureFromChar(ch):\n",
    "    result = np.zeros(feature_size, dtype=np.float32)\n",
    "    result[char_to_idx[ch]] = 1\n",
    "    return result\n",
    "\n",
    "def charFromFeature(v):\n",
    "    return idx_to_char[v.tolist().index(1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test featureFromChar and charFromFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first character is [\n",
      "v = 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 len = 75\n",
      "charFromFeature = [\n"
     ]
    }
   ],
   "source": [
    "print(\"first character is {}\".format(kjv[0]))\n",
    "v = featureFromChar(kjv[0])\n",
    "print(\"v = {} len = {}\".format(' '.join([str(x) for x in v]), len(v)))\n",
    "print(\"charFromFeature = {}\".format(charFromFeature(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review of Recurrent Neural Networks\n",
    "First we calculate what to forget\n",
    "$$f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f )$$\n",
    "Then we calculate what to learn\n",
    "$$i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i )$$\n",
    "$$\\tilde C_t = tanh(W_C \\cdot [h_{t-1},x_t] + b_C )$$\n",
    "Then we update memory\n",
    "$$C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde C_t$$\n",
    "Finally we calculate output\n",
    "$$o_t = \\sigma (W_o [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\cdot tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KJVDeserializer(cntk.io.UserDeserializer):\n",
    "    def __init__(self, data, streams):\n",
    "        super(KJVDeserializer, self).__init__()\n",
    "        self._chunksize = 5000\n",
    "        self._data = data\n",
    "        \n",
    "        # Create the information about streams\n",
    "        # based on the user provided data\n",
    "        self._streams = [cntk.io.StreamInformation(s['name'], i, 'dense', np.float32, s['shape'])\n",
    "                         for i, s in enumerate(streams)]\n",
    "        \n",
    "        # Define the number of chunks based on the file size\n",
    "        self._num_chunks = int(math.ceil((len(data)-1)/self._chunksize))\n",
    "        \n",
    "    def stream_infos(self):\n",
    "        return self._streams\n",
    "\n",
    "    def num_chunks(self):\n",
    "        return self._num_chunks\n",
    "\n",
    "    # Ok, let's actually get the work done\n",
    "    def get_chunk(self, chunk_id):\n",
    "        start = chunk_id * self._chunksize\n",
    "        end = ((chunk_id + 1) * self._chunksize)+1\n",
    "        if (chunk_id == self._num_chunks):\n",
    "            end = len(self._data)\n",
    "        data = self._data[start:end]\n",
    "        datalen = len(data) - 1\n",
    "        result = {}\n",
    "        for i, stream in enumerate(self._streams):\n",
    "            result[stream.m_name] = np.array([featureFromChar(self._data[j+i]) for j in range(datalen)], dtype=np.float32)\n",
    "            \n",
    "        return result\n",
    "\n",
    "def create_reader(data, is_training=False):\n",
    "    d = KJVDeserializer(data=data, streams=[dict(name='features', shape=(feature_size,)), dict(name='labels', shape=(feature_size,))])\n",
    "    return cntk.io.MinibatchSource([d], randomize=False, max_sweeps= cntk.io.INFINITELY_REPEAT if is_training else 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test KJVDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing train set\n",
      ".................................................................................\n",
      ".................................................................................\n",
      ".................................................................................\n",
      ".................................................................................\n",
      ".............\n",
      "Total number of samples 4323056, speed 136081.478813 samples per second\n",
      "Testing test set\n",
      "\n",
      "Total number of samples 9496, speed 81909.925418 samples per second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_reader(mbs):\n",
    "    total_num_samples = 0\n",
    "    dots = 0\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        mb = mbs.next_minibatch(128)\n",
    "        if not mb:\n",
    "            break\n",
    "    \n",
    "        total_num_samples += mb[mbs.streams.features].number_of_samples\n",
    "        if total_num_samples % 12800 == 0:\n",
    "            sys.stdout.write('.')\n",
    "            dots += 1\n",
    "            if dots > 80:\n",
    "                sys.stdout.write('\\n')\n",
    "                dots = 0\n",
    "    end = time.time()\n",
    "    sys.stdout.write('\\n')\n",
    "    print('Total number of samples %d, speed %f samples per second' % (total_num_samples, total_num_samples/(end-start)))    \n",
    "    \n",
    "print(\"Testing train set\")\n",
    "test_reader(create_reader(train))\n",
    "print(\"Testing test set\")\n",
    "test_reader(create_reader(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#           cntk.layers.Recurrence(\n",
    "#               step_function, \n",
    "#               go_backwards=default_override_or(False), \n",
    "#               initial_state=default_override_or(0), \n",
    "#               return_full_state=False, \n",
    "#               name='')\n",
    "#           cntk.layers.RecurrenceFrom(\n",
    "#               step_function, \n",
    "#               go_backwards=default_override_or(False), \n",
    "#               return_full_state=False, \n",
    "#               name='')\n",
    "#           cntk.layers.Fold(\n",
    "#               folder_function, \n",
    "#               go_backwards=default_override_or(False), \n",
    "#               initial_state=default_override_or(0), \n",
    "#               return_full_state=False, \n",
    "#               name='')\n",
    "#           cntk.layers.UnfoldFrom(\n",
    "#               generator_function, \n",
    "#               until_predicate=None, \n",
    "#               length_increase=1, \n",
    "#               name='')\n",
    "        \n",
    "        \n",
    "#           cntk.layers.LSTM(\n",
    "#               shape, \n",
    "#               cell_shape=None, \n",
    "#               activation=default_override_or(tanh), \n",
    "#               use_peepholes=default_override_or(False),\n",
    "#               init=default_override_or(glorot_uniform()), \n",
    "#               init_bias=default_override_or(0),\n",
    "#               enable_self_stabilization=default_override_or(False),\n",
    "#               name='')\n",
    "#           cntk.layers.GRU(\n",
    "#               shape, \n",
    "#               cell_shape=None, \n",
    "#               activation=default_override_or(tanh),\n",
    "#               init=default_override_or(glorot_uniform()), \n",
    "#               init_bias=default_override_or(0),\n",
    "#               enable_self_stabilization=default_override_or(False),\n",
    "#               name='')\n",
    "#           cntk.layers.RNNStep(\n",
    "#               shape, \n",
    "#               cell_shape=None, \n",
    "#               activation=default_override_or(sigmoid),\n",
    "#               init=default_override_or(glorot_uniform()),\n",
    "#               init_bias=default_override_or(0),\n",
    "#               enable_self_stabilization=default_override_or(False),\n",
    "#               name='')\n",
    "\n",
    "def create_model(x):\n",
    "#    with cntk.layers.default_options(initial_state = 0.1):\n",
    "    m = cntk.layers.Sequential([\n",
    "        cntk.layers.Stabilizer(),\n",
    "        cntk.layers.Recurrence(cntk.layers.LSTM(feature_size), name='RecurrenceLayer1'),\n",
    "        cntk.sequence.last,\n",
    "#        cntk.layers.Dropout(0.2, name='DropoutLayer'),\n",
    "        cntk.layers.Dense(feature_size, activation=None, name='DenseLayer')\n",
    "    ])\n",
    "    return m(x)\n",
    "\n",
    "# Single RNN LSTM\n",
    "# (Min Loss: 3.4094 Min Error: 0.64)\n",
    "\n",
    "# Single RNN LSTM isolating last sequence                       \n",
    "# (Min Loss: 3.4094 Min Error: 0.65)\n",
    "\n",
    "# Single RNN LSTM isolating last sequence into a Dense Layer    \n",
    "# (Min Loss: 1.9809 Min Error: 0.58)\n",
    "\n",
    "# Single RNN GRU                                                \n",
    "# (Min Loss: 3.0813 Min Error: 0.60)\n",
    "\n",
    "# Single RNN GRU isolating last sequence                        \n",
    "# (Min Loss: 3.0812 Min Error: 0.60)\n",
    "\n",
    "# Single RNN GRU isolating last sequence into a Dense Layer     \n",
    "# (Min Loss: 1.9719 Min Error: 0.58)\n",
    "\n",
    "# Single RNN RNNStep                                            \n",
    "# (Min Loss: 3.7362 Min Error: 0.61)\n",
    "# Single RNN RNNStep isolating last sequence                    \n",
    "# (Min Loss: 3.7349 Min Error: 0.61)\n",
    "\n",
    "# Single RNN RNNStep isolating last sequence into a Dense Layer \n",
    "# (Min Loss: 2.0307 Min Error: 0.58)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 51001 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "dynamic_feature_axes = [cntk.Axis.default_batch_axis(), cntk.Axis.default_dynamic_axis()]\n",
    "features = cntk.input_variable(shape=(feature_size, ), dynamic_axes=dynamic_feature_axes)\n",
    "\n",
    "#seq_axis = cntk.Axis('inputAxis')\n",
    "#features = cntk.sequence.input_variable(shape=(feature_size, ), sequence_axis=seq_axis)\n",
    "\n",
    "model = create_model(features)\n",
    "\n",
    "cntk.logging.log_number_of_parameters(model)\n",
    "\n",
    "labels = cntk.input_variable(shape=feature_size, dynamic_axes=model.dynamic_axes)\n",
    "#labels = cntk.sequence.input_variable(shape=feature_size, sequence_axis=seq_axis)\n",
    "\n",
    "loss = cntk.cross_entropy_with_softmax(model, labels)\n",
    "label_error = cntk.classification_error(model, labels)\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "minibatch_size = 100\n",
    "samples_per_sweep = len(train)\n",
    "sweeps = 50000\n",
    "minibatches_to_train = (samples_per_sweep * sweeps) / minibatch_size\n",
    "\n",
    "lr_schedule = cntk.learning_rate_schedule(learning_rate, cntk.UnitType.minibatch)\n",
    "\n",
    "#learner = cntk.sgd(model.parameters, lr_schedule)\n",
    "\n",
    "momentum_time_constant = cntk.momentum_as_time_constant_schedule(minibatch_size / -math.log(0.9)) \n",
    "learner = cntk.fsadagrad(model.parameters, \n",
    "                      lr = lr_schedule, \n",
    "                      momentum = momentum_time_constant, \n",
    "                      unit_gain = True)\n",
    "\n",
    "trainer = cntk.Trainer(model, (loss, label_error), [learner])\n",
    "\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb % frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}\".format(mb, training_loss, eval_error))\n",
    "        \n",
    "    return mb, training_loss, eval_error\n",
    "\n",
    "reader_train = create_reader(train, True)\n",
    "\n",
    "input_map = {\n",
    "    labels: reader_train.streams.labels,\n",
    "    features: reader_train.streams.features\n",
    "}\n",
    "\n",
    "training_progress_output_freq = 5000\n",
    "\n",
    "out = cntk.softmax(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 4.3236, Error: 1.00\n",
      "-----------\n",
      "10N,UT;2]vVGtr\n",
      "ruZ\n",
      "auC62fDcgQp99jSvZ8]icFnZlOS]yU2a,zv43Z(nGyoiI:iDueHr!A:qtqWe)AP,z8H'o3LxIEWl!?g13zJEEyYj9WC;2aZbtBDi-3m8iZ7pZDVsb]-',CRZRSIRh,?18W]sWp!PiG6)P'chrFnI26.]nTPEv7Bh'H)3n-QsyQi)gxt.v4vYD0xJrR3LFPe7Mgi5!AnLzWsK).f WxsbeF7Y0c:59FnY31p.hjotI?RcK mn,Wkww7qsA7[Ho6c\n",
      "tpac5ZqLJVFPedsoC:8I;l4f AIyGzz2W-jclzln;Kf1ZWzNdaRC1Q7 NkBNE'O?vJ\n",
      "JrGkCcxz[VYZsPS-Ly5;0Q1dVRyMJkBZFdm\n",
      "VK05fzx0ZE:86reA7l(,0t.t07g19iL(FTeJ,ZM6b[Pfwkc-[L:WQmFR5uj,h2npu:i(1[i'tf4WmogrPc'OdAipPKOVQdGj\n",
      "TltVOa'Jf4BH)5o4 nhif,s3Phpdau1kLGGnmIxsC9.H]\n",
      "u(w65Ag\n",
      "FWHKE]Y30xNsRnC4)5VISIti]?CupDtm15:RvuT3KvsIwU?YR?32E.a3DlF!!pfab\n",
      "'AivmyKcCBsxu[(JuIy?WU17FImzM kfbAOk,hEm9bCP7HnBtkIC6o!]3P[7G[q))UEHu9tEk],eU:BkDiD7-HFunMe.!y89ZSDQOnlbCH?P-Ntkjxo2qJvBtDU]t3mKdIsnm8nNxMvT.:uU0l?l TBr9U(qLwRdW1ib0F4K.sO?x(vnZb5TeRI2)A s2]SmPc3Uzkh]n-(o4)2warMPij5,50-8qxU]H.MsPyFzyCeDbCu49[]POHUicnx,MOusAT0izM(jnEKlfwr]VKVOCP  .':K(kQ20dq\n",
      "d6IHen709sRSGZiDDGs4tIBZ60b24x!IYR;3b0Cvsgt0,f?2L9(q[o]Zth-BfW(ydB:\n",
      "GNHN.?kr.0feNR8nrJ62ZoeF,krVuL-Th'osterYA.JIs\n",
      "-----------\n",
      "Minibatch: 5000, Loss: 2.5500, Error: 0.62\n",
      "Minibatch: 10000, Loss: 2.5049, Error: 0.62\n",
      "Minibatch: 15000, Loss: 2.4939, Error: 0.62\n",
      "Minibatch: 20000, Loss: 2.4877, Error: 0.62\n",
      "Minibatch: 25000, Loss: 2.4835, Error: 0.62\n",
      "Minibatch: 30000, Loss: 2.4804, Error: 0.62\n",
      "Minibatch: 35000, Loss: 2.4780, Error: 0.62\n",
      "Minibatch: 40000, Loss: 2.4760, Error: 0.62\n",
      "Minibatch: 45000, Loss: 2.1495, Error: 0.63\n",
      "Minibatch: 50000, Loss: 2.1486, Error: 0.63\n",
      "-----------\n",
      "1: the tfod God Godatheigheawhertheafise Kid athesad God.\n",
      "1 the aghe five Moru bof th.\n",
      "ve.\n",
      "wand llyighe bs Goveread, on gome amathieave t Soro.\n",
      "\n",
      "1:31:4 the]\n",
      "t bethen And: g, thee butearmeas\n",
      "cher mat, rs le he th Lengnderng asar creay g Gofre eeas e ibers asse ther t e op.\n",
      "\n",
      "11:2 am brthel sarithes thine ldandamertecr, owat pl, ore thed whedat Kis, f t y al sorupo thend, her e sicre theag me chtead Lemeave\n",
      "athernd bl el ighed ind eandeld.\n",
      "h fort th, o o thed od And sar theth and sthted lith ftheay amofondes br\n",
      "arthepor thefry, And flir, wheand Bireawhe csantheaved m tithin ealire\n",
      "ube ie thehere upogr e And, thermand, as\n",
      "\n",
      "\n",
      "\n",
      "and th.\n",
      "1:4 for ad walere LORD eeas thes te inther me e athes may e\n",
      "\n",
      "\n",
      "16 An eleerind s, sthan mas\n",
      "15 we he are saryofthe tfand ind thithe\n",
      "thand, methat heveafet:30 d s Ang, s, cret br:2: u Bind her fupe fant, de D und t ghed u frther Sepore crk t find hensth t, LORnd arupodel\n",
      "tovisand Jare t freathith and s ted a, arthe\n",
      "\n",
      "tsaven atint hery wang ind And tht d ar\n",
      "14 And,\n",
      "\n",
      "-----------\n",
      "Minibatch: 55000, Loss: 2.1478, Error: 0.63\n",
      "Minibatch: 60000, Loss: 2.1471, Error: 0.63\n",
      "Minibatch: 65000, Loss: 2.1465, Error: 0.63\n",
      "Minibatch: 70000, Loss: 2.1460, Error: 0.63\n",
      "Minibatch: 75000, Loss: 2.1455, Error: 0.63\n",
      "Minibatch: 80000, Loss: 2.1450, Error: 0.63\n",
      "Minibatch: 85000, Loss: 2.1446, Error: 0.63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4182c58fa001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatches_to_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_training_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_progress_output_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontains_minibatch_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 updated = super(Trainer, self).train_minibatch_overload_for_minibatchdata(\n\u001b[0;32m--> 170\u001b[0;31m                     arguments, device)\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments,\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch_overload_for_minibatchdata\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch_overload_for_minibatchdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch_overload_for_minibatchdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(int(minibatches_to_train)):\n",
    "    data = reader_train.next_minibatch(minibatch_size, input_map=input_map)\n",
    "    trainer.train_minibatch(data)\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq)\n",
    "    if i % 50000 == 0:\n",
    "        print(\"-----------\")\n",
    "        ch = '1'\n",
    "        for j in range(1000):\n",
    "            feature = featureFromChar(ch)\n",
    "            output = out.eval(feature)\n",
    "            sys.stdout.write(ch)\n",
    "            ch = idx_to_char[np.random.choice(range(feature_size), p=output[0].ravel())]\n",
    "        print(\"\\n-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
